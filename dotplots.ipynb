{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neurogenomics Database: Dotplot of entire dataset predictions\n",
    "Author: Nienke Mekkes <br>\n",
    "Date: 11-10-2022. <br>\n",
    "Correspond: n.j.mekkes@umcg.nl <br>\n",
    "\n",
    "## Script: Dotplot of entire dataset predictions\n",
    "Build Dot Plots for each diagnosis category.\n",
    "\n",
    "\n",
    "### Input files:\n",
    "- predictions file\n",
    "- metadata about attributes (for correct order and grouping)\n",
    "- output folder\n",
    "- information about donors\n",
    "- expectations about symptomatology\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script works with a clinical trajectory dictionary pickle. this pickle can be a rules of thumb or a original pickle, which was generated by the script proces_predictions. These pickles were not prefiltered on donorlevel, meaning they can be used by both the paper and the 'total website'. This means that in the original predictions, weird sentences etc. and the 10 attributes that perform poorly are removed. However, donors that you want to be excluded have to be excluded. This can be done in two ways: <br>\n",
    "1. in this script, manually. for example remove donors younger than 21. or donors with the NAD diagnosis, or reassign diagnosis (e.g. a SSA, CON donor NBB xxx needs to become HIV).\n",
    "2. with an input file, for example an excel file that contains minimally one column with donorids, and one column with adiagnosis. diagnosis can be an updated diagnosis (for example, NBB xxxx and diagnosis HIV), or the word 'excluded' when you want to exclude it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_predictions = \"\"\n",
    "path_to_attribute_grouping = \"\"\n",
    "figure_folder = \"\"\n",
    "expected_attributes_path = \"\"\n",
    "general_information = \"\"\n",
    "\n",
    "path_to_cleaned_training_data = \"\"\n",
    "\n",
    "train_plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "import numpy as np; np.random.seed(0)\n",
    "from matplotlib import pyplot as plt \n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scattermap\n",
    "from scattermap import scattermap\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import statsmodels\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "import scipy\n",
    "from helper_functions import permutation_of_individual_test, table_selector\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(statsmodels.__version__)\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(figure_folder):\n",
    "    print('Creating output folder....')\n",
    "    os.makedirs(figure_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_plot == False:\n",
    "    with open(path_to_predictions,\"rb\") as file:\n",
    "        predictions_pickle = pickle.load(file)\n",
    "\n",
    "    d = []\n",
    "    for i,j in zip(predictions_pickle,predictions_pickle.values()):\n",
    "        k = pd.DataFrame.from_dict(j,orient=\"index\")\n",
    "        k[\"DonorID\"] = i\n",
    "        k['Age'] = k.index\n",
    "        d.append(k)\n",
    "\n",
    "    predictions_df =pd.concat(d, ignore_index=True)\n",
    "    display(predictions_df)\n",
    "    print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n",
    "    print(predictions_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_plot == True:\n",
    "    with open(path_to_cleaned_training_data,\"rb\") as file:\n",
    "        predictions_pickle = pickle.load(file)\n",
    "        predictions_df = pd.DataFrame(predictions_pickle)\n",
    "\n",
    "    predictions_df = predictions_df.rename(columns={\"NBB_nr\": \"DonorID\"})\n",
    "    predictions_df.drop(['Year_Sentence_nr'], axis=1,inplace=True,  errors='ignore')\n",
    "    display(predictions_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exclude/change donors for the paper, using general info\n",
    "- under 21\n",
    "- NAD diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_information_df = pd.read_excel(general_information, engine='openpyxl', sheet_name=\"Sheet1\")\n",
    "donors_to_remove = list(general_information_df[general_information_df['paper diagnosis']=='exclude'].DonorID)\n",
    "predictions_df = predictions_df[~predictions_df['DonorID'].isin(donors_to_remove)]\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n",
    "print(len(donors_to_remove))\n",
    "predictions_df['neuropathological_diagnosis'] = predictions_df['DonorID'].map(general_information_df.set_index('DonorID')['paper diagnosis'])\n",
    "display(predictions_df.head())\n",
    "print(sorted(predictions_df['neuropathological_diagnosis'].unique()))\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_plot == False:\n",
    "    non_attribute_columns = ['DonorID','Year','age_at_death','sex',\n",
    "                            'neuropathological_diagnosis','Age'] #'birthyear',,'death_year','year_before_death','sex',\n",
    "if train_plot == True:\n",
    "    non_attribute_columns = ['DonorID','neuropathological_diagnosis','Sentence'] #'birthyear',,'death_year','year_before_death','sex',\n",
    "attributes = [col for col in predictions_df.columns if col not in non_attribute_columns]\n",
    "# display(attributes)\n",
    "print(f\"there are {predictions_df.shape[0]} rows and {len(attributes)} attributes\")\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = predictions_df[predictions_df['DonorID'] == 'NBB 2020-013']\n",
    "foo['Dementia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting future dotplot colors based on domain or grouping\n",
    "The attributes all belong to different 'domains' and subgroups. we color them based on these grouping. <br>\n",
    "\n",
    "We make two dictionaries with the attributes (values) grouped with their domains (keys). One with the finalised names, this dictionary will be used to update the attribute names. The other one has the 'pc_friendly' names, these are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0], sheet_name='90 parameters')\n",
    "attribute_grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### based on grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict_fancy = dict()\n",
    "color_dict_fancy = dict()\n",
    "count = 0\n",
    "\n",
    "colors = {'Aspecific_symptoms':'#ce6dbd',#'Aspecific_symptoms'\n",
    "          'Autonomic_dysfunction':'#b5cf6b',#'Autonomic_dysfunction'\n",
    "         'Cerebral_vestibular_system_dysfunction': '#6b6ecf',#'Cerebral_vestibular_system_dysfunction'\n",
    "         'Changes_in_consciousness_awareness_orientation': '#d6616b',# Changes_in_consciousness_awareness_orientation\n",
    "         'cognitive_and_memory_impairment':'#e7ba52',#'cognitive_and_memory_impairment'\n",
    "          '(Dis)inhibition':'#bd9e39',#'Disinhibition'\n",
    "          'Disturbances_in_mood_behaviour':'#ad494a',#Disturbances_in_mood_behaviour\n",
    "          'Extrapyramidal_signs_symptoms':'#9c9ede',#Extrapyramidal_signs_symptoms\n",
    "          'General_decline':'#a55194',#\n",
    "          'Impaired_mobility':'#393b79',#Mobility_problems\n",
    "          'Motor_deficit':'#5254a3',#Motor_deficit\n",
    "         'oth_signs_symptoms_cortical_dysfunction': '#e7cb94',#oth_signs_symptoms_cortical_dysfunction\n",
    "        'other_psychiatric_signs_symptoms':  '#e7969c',#other_psychiatric_signs_symptoms\n",
    "          'Sensory_deficits':'#8ca252'}#Sensory_deficits]\n",
    "\n",
    "\n",
    "    \n",
    "for attr, group in zip(attribute_grouping.index, attribute_grouping[\"Grouping\"]):\n",
    "    if group not in group_dict_fancy:\n",
    "#         print(f\"{attr} belonging to {group}, grouping is new\")\n",
    "        if not isinstance(group, float):\n",
    "            group_dict_fancy[group] = []\n",
    "            color_dict_fancy[group] = colors[group]\n",
    "#             print(colors[group])\n",
    "#             print(color_dict_fancy)\n",
    "            group_dict_fancy[group].append(attr)\n",
    "            count +=1\n",
    "    else:\n",
    "        group_dict_fancy[group].append(attr)\n",
    "        \n",
    "print(group_dict_fancy, '\\n')\n",
    "print(color_dict_fancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define order of displaying groupings/domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_order = ['Aspecific_symptoms',\n",
    "                'General_decline',\n",
    "                'Extrapyramidal_signs_symptoms',\n",
    "                'Cerebral_vestibular_system_dysfunction',\n",
    "                'Motor_deficit',\n",
    "                'Impaired_mobility',\n",
    "                'Autonomic_dysfunction',\n",
    "                'Sensory_deficits',\n",
    "                'oth_signs_symptoms_cortical_dysfunction',\n",
    "                'cognitive_and_memory_impairment',\n",
    "                '(Dis)inhibition',\n",
    "                'other_psychiatric_signs_symptoms',\n",
    "                'Changes_in_consciousness_awareness_orientation',\n",
    "                'Disturbances_in_mood_behaviour',\n",
    "               ]\n",
    "\n",
    "new_order_fancy = []\n",
    "for x in domain_order:\n",
    "    group_fancy = group_dict_fancy[x]\n",
    "    for attr in group_fancy:\n",
    "        new_order_fancy.append(attr)\n",
    "# new_order_fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These attributes perform poorly and need to be removed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_plot == False:\n",
    "    new_order_fancy.remove('Unspecified disturbed gait patterns')\n",
    "    new_order_fancy.remove('Loss of sympathy / empathy')\n",
    "    new_order_fancy.remove('Headturning sign')\n",
    "    new_order_fancy.remove('Impaired comprehension')\n",
    "    new_order_fancy.remove('Changed behavior/personality')\n",
    "    new_order_fancy.remove('Frontal release signs')\n",
    "    # new_order_fancy\n",
    "\n",
    "    # remove 4 synonyms\n",
    "    new_order_fancy.remove('Ataxia')\n",
    "    new_order_fancy.remove('Lack of initiative')\n",
    "    new_order_fancy.remove('Lack of planning / organization / overview')\n",
    "    new_order_fancy.remove('Cognitive decline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to display the attributes using the official names from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = {}\n",
    "\n",
    "for attr, real_name in zip(attribute_grouping.index, attribute_grouping[\"Attribute\"]):\n",
    "    if not isinstance(real_name, float):\n",
    "        correct_names[real_name] = attr\n",
    "# correct_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.rename(correct_names,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change the attribute order to the one we want to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_from_symptoms_df = predictions_df[non_attribute_columns]\n",
    "attribute_columns_to_sort = predictions_df.loc[:,[i for i in list(predictions_df.columns) if i not in non_attribute_columns]]\n",
    "attribute_columns_to_sort = attribute_columns_to_sort[new_order_fancy]\n",
    "          \n",
    "              \n",
    "# updated symptoms_df, now with the right columns order\n",
    "predictions_df = pd.concat([information_from_symptoms_df, attribute_columns_to_sort], axis=1)\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting groups of diagnoses to display in dotplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to print all hundreds of diagnoses, but make a selection. here we use a dictionary approach to select diagnoses and give them an appropriate abbreviation in one go <br>\n",
    "\n",
    "For the statistical permutation test we have two different approaches: <br>\n",
    "table1: main diagnosis, the background is table 1 without the current diagnosis <br>\n",
    "table 2 t/m 7: the background is table 1 where all diagnoses belonging to the table of interest are removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make the main diagnosis readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symptoms_df['Main_diagnosis'] = symptoms_df['Main_diagnosis'].str.strip('[]').str.strip('\"').str.strip(\"'\").str.replace(' ', '')\n",
    "xs = list(predictions_df.neuropathological_diagnosis.unique())\n",
    "xs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select sentences from the predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_choice = 'table1_p' #fig 4a table3_with_con_p #table2_p #fig 3a table1_P fig sup 5a:table2_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_diagnoses,ordered_diagnoses = table_selector(table_of_choice, predictions_df)\n",
    "print('After selecting for {}, we have {} donors'.format(selected_diagnoses['neuropathological_diagnosis'].unique(),\n",
    "                                                                                    selected_diagnoses['DonorID'].nunique()) )\n",
    "display(ordered_diagnoses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1, _ = table_selector('table1_p', predictions_df)\n",
    "print('After selecting for {}, we have {} donors'.format(table1['neuropathological_diagnosis'].unique(),\n",
    "                                                                                    table1['DonorID'].nunique()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(selected_diagnoses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DonorID\tYear\t\t\tneuropathological_diagnosis\t\n",
    "if train_plot == False:\n",
    "    table1.drop(['Year','age_at_death','sex','Age'], axis=1,inplace=True, errors='ignore')\n",
    "    selected_diagnoses.drop(['Year','age_at_death','sex','Age'], axis=1,inplace=True,  errors='ignore')\n",
    "if train_plot == True:\n",
    "    table1.drop(['Sentence'], axis=1,inplace=True, errors='ignore') ## for training data\n",
    "    selected_diagnoses.drop(['Sentence'], axis=1,inplace=True,  errors='ignore') ## for training data\n",
    "\n",
    "flattened_t1 = table1.groupby(['DonorID','neuropathological_diagnosis'], as_index=False).sum()\n",
    "flattened = selected_diagnoses.groupby(['DonorID','neuropathological_diagnosis'], as_index=False).sum()\n",
    "display(flattened)\n",
    "selected_donorcountpredict = table1['DonorID'].nunique()\n",
    "# print(selected_donorcountpredict)\n",
    "\n",
    "print(flattened['neuropathological_diagnosis'].value_counts())\n",
    "print(flattened_t1['neuropathological_diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_counts = pd.DataFrame(flattened['neuropathological_diagnosis'].value_counts())\n",
    "disease_counts = disease_counts.reindex(ordered_diagnoses)\n",
    "display(disease_counts)\n",
    "print(disease_counts.shape[0])\n",
    "print(disease_counts.loc[['CON']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## endgoal: divide total nr of donors in a diagnosis group by the nr of donors suffering from attribute x\n",
    "## step 1: make boolean df: a donor either has a attribute or not, i dont care about how often.\n",
    "## by setting it to 1, we can sum and see how many donors have this attribute\n",
    "group_ready = flattened.copy()\n",
    "group_ready.loc[:,[i for i in list(predictions_df.columns) if i not in non_attribute_columns]] = group_ready.loc[:,[i for i in list(predictions_df.columns) if i not in non_attribute_columns]].apply(lambda x: [y if y <= 1 else 1 for y in x])\n",
    "proportion_df = group_ready.groupby('neuropathological_diagnosis').sum()\n",
    "\n",
    "## now we want to know the total nr of donors in a diagnosis group:\n",
    "proportion_df['total'] = disease_counts \n",
    "proportion_df=proportion_df.reindex(ordered_diagnoses)\n",
    "# display(proportion_df)\n",
    "\n",
    "## divide by this total and multiple for percentage\n",
    "## ofcourse true percentage is multiplied by 100. we multiply by 300 purely for visualization purposes (== bigger circles)\n",
    "proportion_df.loc[:,[i for i in list(predictions_df.columns) if i not in non_attribute_columns]] = proportion_df.loc[:,[i for i in list(predictions_df.columns) if i not in non_attribute_columns]].div(proportion_df.total, axis=0) * 300\n",
    "proportion_df = proportion_df.drop(['total'],axis=1)\n",
    "display(proportion_df)\n",
    "proportion_df.to_excel(f\"{figure_folder}/{table_of_choice}/percentages.xlsx\")\n",
    "display(proportion_df['Dementia'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate mean value\n",
    "general_mean_multi = flattened.groupby('neuropathological_diagnosis').mean()\n",
    "general_mean_multi = general_mean_multi.reindex(ordered_diagnoses)\n",
    "display(general_mean_multi)\n",
    "print(general_mean_multi.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation testing\n",
    "Only run once, takes a lot of time if you dont have resources. can be skipped if you dont need significane, then do not plot significance in the visualization or you will get an error. Can also be faster if you run less permutations. standard set to 100.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_diagnosis_Pvalues(pd, occurence,mdf,dc,t1o):\n",
    "    \n",
    "    ##Function that is a wrapper around permutation_of_individual_test and saves all P-values\n",
    "    ##Make a diagnosis dictionary with a attrobite dictionary that contains all the P-values \n",
    "    p_values_diagnosis_dictionary = {} \n",
    "    perms = 100000\n",
    "#     temp_pd = pd[0:2]\n",
    "    nr = 1\n",
    "    for d in pd:\n",
    "        ##Print messages\n",
    "        message = '--------------------------------------- \\n \\\n",
    "                   Working on {primary_diagnosis}, {v} out of {len_pd}'.format(primary_diagnosis=d,v=nr,len_pd=len(pd))\n",
    "        print(message)\n",
    "        nr = nr + 1\n",
    "#         now = datetime.now()\n",
    "#         current_time = now.strftime(\"%H:%M:%S\")\n",
    "#         print(\"Current Time =\", current_time)\n",
    "        nr_donors_with_d = int(dc.loc[d]['neuropathological_diagnosis'])\n",
    "        print('diagnosis affects {} donors.'.format(nr_donors_with_d))\n",
    "        # multiproc\n",
    "        p_values_attribute_dictionary = {} \n",
    "        iterable = [attribute_nr for attribute_nr in range(2,occurence.shape[1])]\n",
    "#         print(iterable)\n",
    "        pool = multiprocessing.Pool(multiprocessing.cpu_count()-1)\n",
    "#         mom = 'mean'\n",
    "#         if mom == 'mean':\n",
    "        func = partial(permutation_of_individual_test, d, occurence, mdf, nr_donors_with_d,perms,t1o)\n",
    "#         elif mom == 'median':\n",
    "#             func = partial(permutation_of_individual_test, d,  occurence, mdf, nr_donors_with_d,t1o,m_or_m='median')\n",
    "        res = pool.map(func, iterable)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        p_values_diagnosis_dictionary[d] = res\n",
    "        \n",
    "#         ## without multiproc\n",
    "#         p_values_attribute_dictionary = {} \n",
    "\n",
    "#         for attribute_nr in range(2,occurence.shape[1]): #range(2,6):\n",
    "#             message2 = 'Working on attribute {nr}: {attribute}'.format(nr=attribute_nr-2,\n",
    "#                                                                        attribute=occurence.columns[attribute_nr])\n",
    "#             print(message2)\n",
    "#             p_value = permutation_of_individual_test(d,\n",
    "#                                                      occurence,\n",
    "#                                                      mdf,\n",
    "#                                                      nr_donors_with_d,\n",
    "#                                                      t1o,\n",
    "#                                                      attribute_nr,\n",
    "#                                                      m_or_m='mean')#, donor_diagnosis_list)\n",
    "#             p_values_attribute_dictionary[attribute_nr] = p_value\n",
    "\n",
    "#         p_values_diagnosis_dictionary[d] = p_values_attribute_dictionary\n",
    "    \n",
    "    return p_values_diagnosis_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_plot == False:\n",
    "    table_folder = \"{}/{}\".format(figure_folder,table_of_choice)\n",
    "if train_plot == True:\n",
    "    table_folder = \"{}/{}_training\".format(figure_folder,table_of_choice)\n",
    "print(table_folder)\n",
    "\n",
    "if not os.path.exists(table_folder):\n",
    "    print('Creating output folder....')\n",
    "    os.makedirs(table_folder)\n",
    "    \n",
    "save_permutation = \"{}/p_values_{}.xlsx\".format(table_folder,table_of_choice)\n",
    "print(save_permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# table_of_choice\n",
    "# flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pvalues_dict = identify_diagnosis_Pvalues(ordered_diagnoses, flattened, general_mean_multi, disease_counts, flattened_t1)\n",
    "# prelim = pd.DataFrame(Pvalues_dict)\n",
    "# Pvalues_dataframe = prelim.T\n",
    "# Pvalues_dataframe.columns = list(general_mean_multi.columns)\n",
    "# display(Pvalues_dataframe)\n",
    "\n",
    "# writer = pd.ExcelWriter(save_permutation, engine='xlsxwriter')\n",
    "# Pvalues_dataframe.to_excel(writer)\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading stored permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalues_dataframe = pd.read_excel(save_permutation, engine='openpyxl', index_col=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Pvalues_dataframe=Pvalues_dataframe.reindex(ordered_diagnoses)\n",
    "display(Pvalues_dataframe)\n",
    "print(Pvalues_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correct for multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR_conversion(Pvalues_dataframe):\n",
    "        \n",
    "    ##Function that converts a P-value dataframe to FDR dataframe \n",
    "    import statsmodels.stats.multitest as smt\n",
    "    \n",
    "    Pvalues_list = [] \n",
    "    for index_value in Pvalues_dataframe.index:\n",
    "        Pvalues_list+= Pvalues_dataframe.loc[index_value,:].values.tolist()\n",
    "\n",
    "    FDRvalues_list = smt.multipletests(Pvalues_list, method='fdr_bh', is_sorted= False)[1]    \n",
    "    FDRvalues_array = np.array(FDRvalues_list) \n",
    "    FDRvalues_array = np.reshape(FDRvalues_array, Pvalues_dataframe.shape)\n",
    "    FDR_df = pd.DataFrame(FDRvalues_array, columns= Pvalues_dataframe.columns, index= Pvalues_dataframe.index)\n",
    "    \n",
    "    return FDR_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_df = FDR_conversion(Pvalues_dataframe)\n",
    "FDR_Cutoff = 0.1\n",
    "significance_boolean = (FDR_df < FDR_Cutoff) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our colleagues from the NHB wrote down their expectations for each attribute and diagnosis. we plot these as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Add expected attributes per diagnosis - for plotting \n",
    "expected_attributes_df = pd.read_excel(expected_attributes_path, index_col=0,sheet_name='Updated version 20042022')\n",
    "\n",
    "expected_attributes_df.fillna(0,inplace=True)\n",
    "expected_attributes_df = expected_attributes_df.astype('int')\n",
    "print(pd.unique(expected_attributes_df.values.ravel('K')))\n",
    "expected_attributes_df = expected_attributes_df.rename(columns={'Executive_dysfunction': 'Executive_function_disorder',\n",
    "                                                                'Lack_of_planning_organisation':'Lack_of_planning_organis_overv',\n",
    "                                                               'Unspecified_disturbed_gait_patterns': 'Unspecified_disturbed_gait_patt'})\n",
    "expected_attributes_df = expected_attributes_df.rename(columns={\"Fatique\": \"Fatigue\"})\n",
    "expected_attributes_df= expected_attributes_df.rename(correct_names,axis=1)\n",
    "expected_attributes_df = expected_attributes_df.reindex(ordered_diagnoses)\n",
    "display(expected_attributes_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_merger(df, how=None):\n",
    "    if how == 'sum':\n",
    "        df[\"Loss of coordination\"] = df[[\"Ataxia\", \"Loss of coordination\"]].sum(axis=1)\n",
    "        df[\"Apathy / inertia\"] = df[[\"Apathy / inertia\", \"Lack of initiative\"]].sum(axis=1)\n",
    "        df[\"Dementia\"] = df[[\"Dementia\", \"Cognitive decline\"]].sum(axis=1)\n",
    "        df[\"Executive function disorders\"] = df[[\"Executive function disorders\", \"Lack of planning / organization / overview\"]].sum(axis=1)\n",
    "    elif how == 'max':\n",
    "        df[\"Loss of coordination\"] = df[[\"Ataxia\", \"Loss of coordination\"]].max(axis=1)\n",
    "        df[\"Apathy / inertia\"] = df[[\"Apathy / inertia\", \"Lack of initiative\"]].max(axis=1)\n",
    "        df[\"Dementia\"] = df[[\"Dementia\", \"Cognitive decline\"]].max(axis=1)\n",
    "        df[\"Executive function disorders\"] = df[[\"Executive function disorders\", \"Lack of planning / organization / overview\"]].max(axis=1)\n",
    "    df.drop(['Lack of initiative','Cognitive decline','Ataxia',\"Lack of planning / organization / overview\"], axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "if train_plot == False:\n",
    "    expected_attributes_df = synonym_merger(expected_attributes_df, how='max') ## turn off ## for training data\n",
    "expected_attributes_df = expected_attributes_df[list(general_mean_multi.columns)]\n",
    "expected_attributes_df = expected_attributes_df.fillna(0)\n",
    "display(expected_attributes_df)\n",
    "print(expected_attributes_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all dataframes are nice, now we calculate bar size based on these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERTICAL BARPLOT\n",
    "attribute_bar = pd.DataFrame(flattened[list(general_mean_multi.columns)].sum(),columns=['Attribute'])\n",
    "attribute_bar['freq']=(attribute_bar['Attribute']*-2)/attribute_bar['Attribute'].max()\n",
    "print('attributes min max')\n",
    "print(attribute_bar['Attribute'].max())\n",
    "print(attribute_bar['Attribute'].min())\n",
    "print(attribute_bar['freq'].max())\n",
    "print(attribute_bar['freq'].min())\n",
    "display(attribute_bar)\n",
    "freq =attribute_bar['freq'].tolist()\n",
    "\n",
    "# Get positions for attribute barplots\n",
    "positions = np.arange(start=.5, stop=.5*len(list(general_mean_multi.columns))*2, step=1)\n",
    "print(len(positions))\n",
    "\n",
    "### HORIZONTAL BARPLOT\n",
    "disease_counts['freq']= (disease_counts['neuropathological_diagnosis']*2)/disease_counts['neuropathological_diagnosis'].max()\n",
    "print('diagnoses max min')\n",
    "print(disease_counts['neuropathological_diagnosis'].max())\n",
    "print(disease_counts['neuropathological_diagnosis'].min())\n",
    "print(disease_counts['freq'].max())\n",
    "print(disease_counts['freq'].min())\n",
    "display(disease_counts)\n",
    "prop_freq_diag = disease_counts['freq'].tolist()\n",
    "print(prop_freq_diag)\n",
    "\n",
    "diag_pos = np.arange(start=.5, stop=.5*(len(prop_freq_diag))*2, step=1)\n",
    "print(diag_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_colors = []\n",
    "\n",
    "for col in list(general_mean_multi.columns):\n",
    "#     print(col)\n",
    "    for g in group_dict_fancy:\n",
    "#         print(g)\n",
    "        if col in group_dict_fancy[g]:\n",
    "#             print('grouping: {}'.format(g))\n",
    "            added_colors.append(color_dict_fancy[g])\n",
    "#         print('\\n')\n",
    "#         print(col)\n",
    "\n",
    "print(len(added_colors))\n",
    "print(added_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(len(proportion_df.columns)/2)+2\n",
    "if train_plot == False:\n",
    "    num = 80\n",
    "if train_plot == True:\n",
    "    num = 90 ## for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first40_colnames = proportion_df.columns[0:num]\n",
    "first40_colnames = proportion_df.columns\n",
    "first40_colors = added_colors[0:num]\n",
    "first40_colors =  added_colors\n",
    "print(first40_colnames)\n",
    "print(len(first40_colnames))\n",
    "print(num)\n",
    "# print(proportion_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\", font_scale=1.5, rc={'axes.facecolor':'#F0E6EB', \"grid.linestyle\": \"-\",\"grid.color\": '#b0b0b0'})\n",
    "\n",
    "##PLOT \n",
    "w = disease_counts.shape[0]/1\n",
    "w = (3+ disease_counts.shape[0]/1)-4\n",
    "h = 2+num/3\n",
    "w = 5.5\n",
    "h= 30\n",
    "fig = plt.figure(figsize=(w,h ),dpi=200)\n",
    "print('figsize:',w,h)\n",
    "# fig = plt.figure(figsize=(15, 20))\n",
    "ax1 = plt.subplot2grid((9,9), (0, 0), colspan=8,rowspan=9)\n",
    "\n",
    "# #Dots for expected attributes per disease\n",
    "# ax1 = scattermap(expected_attributes_df[first40_colnames].T,\n",
    "#                 marker='o',\n",
    "#                 marker_size=proportion_df[first40_colnames].T*expected_attributes_df[first40_colnames].T * 1.3,\n",
    "#                 cmap=\"Oranges\",\n",
    "#                 cbar=False,ax=ax1)\n",
    "\n",
    "##Dots for proportions and averages \n",
    "ax1 = scattermap(general_mean_multi[first40_colnames].T,\n",
    "                cmap=\"YlGnBu\",\n",
    "                marker_size=proportion_df[first40_colnames].T,\n",
    "                ax=ax1,\n",
    "                vmax=5,\n",
    "                 linecolor = 'black',\n",
    "                 linewidths = 0.2,\n",
    "                 \n",
    "                cbar_kws={\"shrink\": .5})#cbar_kws = {\"orientation\": \"horizontal\", \"pad\":0.02}\n",
    "#                 cbar_kws = dict(use_gridspec=True,location=\"right\"))#,\n",
    "#                 cbar=False)\n",
    "\n",
    "#Significance \n",
    "ax1 = scattermap(significance_boolean[first40_colnames].T,\n",
    "                marker='*',\n",
    "                marker_size=significance_boolean[first40_colnames].T * 100,\n",
    "                cbar=False,\n",
    "                ax=ax1,\n",
    "                 linecolor = 'black',\n",
    "                 linewidths = 0.2,\n",
    "                cmap=\"Wistia\")\n",
    "\n",
    "# x axis on top\n",
    "ax1.xaxis.tick_top() \n",
    "ax1.xaxis.set_label_position('top')\n",
    "# plt.xticks(rotation=75) (old, no subplot)\n",
    "ax1.tick_params('x', labelrotation=90)\n",
    "\n",
    "# Add frequencies of attributes as barplot to y-axis\n",
    "ax1.barh(list(positions)[:num], freq[0:num], 0.6, alpha=1, color=first40_colors,edgecolor = \"none\")\n",
    "# plt.axvline(x=0, color='k') (old, no subplot)\n",
    "ax1.axvline(x=0, color='k')\n",
    "ax1.axhline(0, color='k')\n",
    "\n",
    "#plt.xlim(attribute_bar['freq'].min()-0.1,disease_counts.shape[0])(old, no subplot)\n",
    "ax1.set_xlim([attribute_bar['freq'].min()-0.1,disease_counts.shape[0]])\n",
    "\n",
    "# Add frequencies of diagnosis as barplot to x-axis\n",
    "ax1.bar(diag_pos, prop_freq_diag, 0.6,color='#41b6c4',bottom=num,edgecolor = \"none\")\n",
    "\n",
    "# plt.ylim(0,80+disease_counts['freq'].max()+0.1)(old, no subplot)\n",
    "ax1.set_ylim([0,num+disease_counts['freq'].max()+0.1])\n",
    "\n",
    "plt.title('{}: {} to {} diagnoses, {} to {} attributes, for {} donors'.format(table_of_choice,\n",
    "                                                                             disease_counts['neuropathological_diagnosis'].min(),\n",
    "                                                                             disease_counts['neuropathological_diagnosis'].max(),\n",
    "                                                                             attribute_bar['Attribute'].min(),\n",
    "                                                                             attribute_bar['Attribute'].max(),\n",
    "                                                                             selected_donorcountpredict))\n",
    "\n",
    "## same size legend\n",
    "ax2 = plt.subplot2grid((9, 9), (3, 8),colspan=1,rowspan=1)\n",
    "x = [2,3,4,5]\n",
    "y = [2,3,4,5]\n",
    "a2 = [75,150,225,300]\n",
    "\n",
    "sc = ax2.scatter(x, y, s=a2, alpha=0.5,c='white')\n",
    "L = ax2.legend(*sc.legend_elements(\"sizes\"),loc='center left', bbox_to_anchor=(1, 0.7),frameon=False)\n",
    "L.get_texts()[0].set_text('25%')\n",
    "L.get_texts()[1].set_text('50%')\n",
    "L.get_texts()[2].set_text('75%')\n",
    "L.get_texts()[3].set_text('100%')\n",
    "\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3 = plt.subplot2grid((9, 9), (2, 8),colspan=1,rowspan=1)\n",
    "x2 = [2]\n",
    "x3 = [2]\n",
    "y2 = [2]\n",
    "y3 = [2]\n",
    "a3 = [300]\n",
    "t = ['significance','expected']\n",
    "ax3.scatter(x2, y2, s=a3, alpha=1,c='#E59400', marker='*')\n",
    "ax3.scatter(x3, y3, s=a3, alpha=1,facecolors='none',edgecolors='#E59400')\n",
    "ax3.legend(t, loc='center left',bbox_to_anchor=(1, 0.3),frameon=False)\n",
    "ax3.set_ylim([0,1])\n",
    "ax3.set_xlim([0,1])\n",
    "ax3.axis('off')\n",
    "\n",
    "print('based on table {}'.format(table_of_choice))\n",
    "print(disease_counts.shape[0])\n",
    "print('{} to {} diagnosis, {} to {} attributes'.format(disease_counts['neuropathological_diagnosis'].min(),\n",
    "                                                         disease_counts['neuropathological_diagnosis'].max(),\n",
    "                                                         attribute_bar['Attribute'].min(),\n",
    "                                                         attribute_bar['Attribute'].max() ))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}/dotplot_{}.png\".format(table_folder,table_of_choice), bbox_inches=\"tight\")\n",
    "plt.savefig(\"{}/dotplot_{}.pdf\".format(table_folder,table_of_choice), bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_testing_of_dataframes(expected_attributes_df, significance_boolean_df):\n",
    "    ##Function to calculate the Chi-square statistic to deteremine the overlap between the significance boolean dataframe and expected dataframe\n",
    "    import scipy\n",
    "    ###Chi-square testing\n",
    "    boolean_matrix = (expected_attributes_df == 1) & (significance_boolean_df ==1)\n",
    "    category00 = np.sum(np.sum((boolean_matrix * 1)))\n",
    "    boolean_matrix = (expected_attributes_df == 0) & (significance_boolean_df ==1)\n",
    "    category01 = np.sum(np.sum((boolean_matrix * 1)))\n",
    "    boolean_matrix = (expected_attributes_df == 1) & (significance_boolean_df ==0)\n",
    "    category10 = np.sum(np.sum((boolean_matrix * 1)))\n",
    "    boolean_matrix = (expected_attributes_df == 0) & (significance_boolean_df ==0)\n",
    "    category11 = np.sum(np.sum((boolean_matrix * 1)))\n",
    "    observations = np.array([[category00,category01], [category10, category11]])\n",
    "    chi2, p, dof, expected = scipy.stats.chi2_contingency(observations)\n",
    "    print(f\"chi2 statistic:   {chi2:.5g}\")\n",
    "    print(f\"p-value:      {p:.5g}\")\n",
    "    print(f\"degrees of freedom: {dof}\")\n",
    "    print()\n",
    "    print(\"expected frequencies:\")\n",
    "    print(expected)\n",
    "    print()\n",
    "    print(\"observed frequencies:\")\n",
    "    print(observations)\n",
    "    \n",
    "chi_square_testing_of_dataframes(expected_attributes_df, significance_boolean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_history",
   "language": "python",
   "name": "clinical_history"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
